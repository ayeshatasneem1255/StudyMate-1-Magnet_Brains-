# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/140azKMFJuU8fNc-ORwFw5BlLGVkS02hI
"""

# ============================
# üöÄ StudyMate ‚Äì PDF Q&A System
# IBM Granite 3.3-2B Instruct + FAISS + Gradio
# ============================

# Install required packages
!pip install transformers accelerate sentence-transformers faiss-cpu pymupdf gradio -q

import fitz  # PyMuPDF
import faiss
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import gradio as gr
import numpy as np

print("üì¶ Loading models... This may take a few minutes.")

# ----------------------------------------
# 1Ô∏è‚É£ Load Embedding Model & LLM (Granite)
# ----------------------------------------

embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
print("‚úÖ Embedding model loaded")

model_name = "ibm-granite/granite-3.3-2b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
llm = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
print("‚úÖ LLM loaded")

# -----------------------------
# 2Ô∏è‚É£ PDF ‚Üí Text ‚Üí Chunking
# -----------------------------
def extract_text_from_pdf(pdf_path):
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()
        return text
    except Exception as e:
        return f"Error reading PDF: {str(e)}"


def chunk_text(text, chunk_size=400):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i+chunk_size])
        if chunk.strip():  # Only add non-empty chunks
            chunks.append(chunk)
    return chunks


# -------------------------------------
# 3Ô∏è‚É£ Create FAISS Vector DB
# -------------------------------------
faiss_index = None
text_chunks = []
processed_files = []


def build_faiss(chunks):
    global faiss_index, text_chunks

    text_chunks = chunks
    embeddings = embed_model.encode(chunks, show_progress_bar=False)

    # Convert to numpy array with correct dtype
    embeddings = np.array(embeddings).astype('float32')

    dim = embeddings.shape[1]
    faiss_index = faiss.IndexFlatL2(dim)
    faiss_index.add(embeddings)


# -------------------------------------
# 4Ô∏è‚É£ Semantic Search + LLM Answer
# -------------------------------------
def answer_question(query):
    if faiss_index is None or len(text_chunks) == 0:
        return "‚ö†Ô∏è Please upload and process PDF files first."

    if not query or query.strip() == "":
        return "‚ö†Ô∏è Please enter a question."

    try:
        # Convert question ‚Üí embedding
        q_emb = embed_model.encode([query], show_progress_bar=False)
        q_emb = np.array(q_emb).astype('float32')

        # Search FAISS
        D, I = faiss_index.search(q_emb, k=3)
        retrieved = "\n\n".join([text_chunks[i] for i in I[0] if i < len(text_chunks)])

        # Prompt to LLM
        prompt = f"""You are an academic assistant. Answer the question using ONLY the provided context.

Context:
{retrieved}

Question:
{query}

Answer:"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)

        # Move inputs to the same device as model
        device = next(llm.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        output = llm.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

        answer = tokenizer.decode(output[0], skip_special_tokens=True)

        # Extract only the answer part (after "Answer:")
        if "Answer:" in answer:
            answer = answer.split("Answer:")[-1].strip()

        return answer

    except Exception as e:
        return f"‚ùå Error generating answer: {str(e)}"


# -------------------------------------
# 5Ô∏è‚É£ Process Multiple PDFs
# -------------------------------------
def process_pdfs(pdf_files):
    global processed_files

    if pdf_files is None or len(pdf_files) == 0:
        return "‚ö†Ô∏è Please upload at least one PDF file.", ""

    all_chunks = []
    processed_files = []
    file_info = []

    for pdf_file in pdf_files:
        try:
            # Extract text
            text = extract_text_from_pdf(pdf_file.name)

            if text.startswith("Error"):
                file_info.append(f"‚ùå {pdf_file.name}: {text}")
                continue

            # Chunk text
            chunks = chunk_text(text)
            all_chunks.extend(chunks)
            processed_files.append(pdf_file.name.split('/')[-1])

            file_info.append(f"‚úÖ {pdf_file.name.split('/')[-1]}: {len(chunks)} chunks extracted")

        except Exception as e:
            file_info.append(f"‚ùå {pdf_file.name}: {str(e)}")

    if len(all_chunks) == 0:
        return "‚ùå No valid content extracted from PDFs.", ""

    # Build FAISS index
    try:
        build_faiss(all_chunks)
        summary = f"‚úÖ **Processing Complete!**\n\n"
        summary += f"üìÑ Files processed: {len(processed_files)}\n"
        summary += f"üìä Total chunks: {len(all_chunks)}\n\n"
        summary += "**File Details:**\n" + "\n".join(file_info)

        files_list = "üìö **Loaded PDFs:**\n" + "\n".join([f"‚Ä¢ {f}" for f in processed_files])

        return summary, files_list

    except Exception as e:
        return f"‚ùå Error building search index: {str(e)}", ""


# -------------------------------------
# 6Ô∏è‚É£ Enhanced Gradio UI
# -------------------------------------
with gr.Blocks(theme=gr.themes.Soft(), title="StudyMate AI") as interface:

    gr.Markdown("""
    # üìö StudyMate ‚Äì AI-Powered PDF Q&A
    ### Powered by IBM Granite 3.3-2B Instruct

    Upload up to 2 PDF files and ask questions about their content!
    """)

    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### üì§ Step 1: Upload PDFs")
            pdf_input = gr.File(
                label="Upload PDF Files (Max 2)",
                file_count="multiple",
                file_types=[".pdf"],
                type="filepath"
            )
            process_btn = gr.Button("üîÑ Process PDFs", variant="primary", size="lg")
            process_output = gr.Markdown(label="Processing Status")
            loaded_files = gr.Markdown(label="Loaded Files")

        with gr.Column(scale=1):
            gr.Markdown("### üí¨ Step 2: Ask Questions")
            question = gr.Textbox(
                label="Your Question",
                placeholder="e.g., What are the main topics discussed in the document?",
                lines=3
            )
            ask_btn = gr.Button("ü§î Get Answer", variant="primary", size="lg")
            answer = gr.Textbox(
                label="Answer",
                lines=10,
                placeholder="Answer will appear here..."
            )

    gr.Markdown("""
    ---
    üí° **Tips:**
    - Upload clear, text-based PDFs for best results
    - Ask specific questions for more accurate answers
    - The AI uses only the uploaded documents to answer
    """)

    # Event handlers
    process_btn.click(
        process_pdfs,
        inputs=pdf_input,
        outputs=[process_output, loaded_files]
    )

    ask_btn.click(
        answer_question,
        inputs=question,
        outputs=answer
    )

    question.submit(
        answer_question,
        inputs=question,
        outputs=answer
    )

print("\nüöÄ Launching interface...")
interface.launch(debug=True, share=True)